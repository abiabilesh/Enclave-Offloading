%     File: VTthesis_template.tex
%     Created: Thu Mar 24 11:00 AM 2016 EDT
%     Last Change: Thursday, December 19, 2019
%     Author: Alan M. Lattimer, VT
%	  With modifications by Carrie Cross, Robert Browder, and LianTze Lim. 
%
% This template is designed to operate with XeLaTeX.
%
% All elements in the Title, Abstract, and Keywords MUST be formatted as text and NOT as math.
%
%Further instructions for using this template are embedded in the document. Additionally, there are comments at the end of the file that give suggestions on writing your thesis.  
%
%In addition to the standard formatting options, the following options are defined for the VTthesis class: proposal, prelim, doublespace, draft. 

\documentclass[article, doublespace,nopageskip]{VTthesis} % nopageskip - Removes arbitrary blank pages.

% Using the following header instead will create a draft copy of your thesis
%\documentclass[doublespace,draft]{VTthesis}

% The lipsum package is just included to put dummy text in the document in order to demonstrate page headers and table of contents behavior. You should remove it once you begin writing your actual thesis or dissertation.
\usepackage{lipsum}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{float}
\usepackage{afterpage}
\usepackage{makecell}
\usepackage[export]{adjustbox}


% Define a variable
\newcommand{\monitor}{HARES }

% Title of your thesis
\title{Enclave Offloading}

% You should include 3-5 keywords, separated by commas
\keywords{Some Keywords, Subject matter, etc.}

% Your name, including middle initial(s)
\author{Alan M. Lattimer}

% Change this to your program, e.g. Physics, Civil Engineering, etc.
\program{Mathematics} 

% Change this to your degree, e.g. Master of Science, Master of Art, etc.
\degree{Doctor of Philosophy} 

% This should be your defense date:
\submitdate{March 23, 2016} 

% Committee members. Only have five readers and one chair available.
% Only use the ones you need and don't include the ones you don't need.
% You can also declare a Co-advisor. If you do, the principal and co-advisors
% will be listed as co-advisors on the title page.  Per the VT ETD standards, 
% you should not include titles or educational qualifications such as PhD or Dr.
% You should, however, include middle initials if possible.
\principaladvisor{John Q. Williams}
%\coadvisor{Vicente Esparza}
\firstreader{Janet K. Martin}
\secondreader{Kara M. Jones}
\thirdreader{James Smith}
%\fourthreader{Fourth Committee Member}
%\fifthreader{Fifth Committee Member}

% The dedication and acknowledgement pages are optional. Comment them out to remove them.
\dedication{This is where you put your dedications.}
\acknowledge{This is where you put your acknowledgments.}

% The abstract is required.
\abstract{Give a brief description of your thesis here.}
%\abstract{\lipsum [1-4]}

% The general audience abstract is required. There are currently no word limits.
\abstractgenaud{You are also required as of Spring 2016 to include a general audience abstract. This should be geared towards individuals outside of your field that may be reading seeking information about your work. You should avoid language that is particular to your field and clearly define any terms that may have special meaning in your discipline.}

\begin{document}
% The following lines set up the front matter of your thesis or dissertation and are required to ensure proper formatting per the VT ETD standards. 
  \frontmatter
  \maketitle
  \tableofcontents

% The list of figures and tables are now optional per the official ETD standards.  Unless you have a very good reason for removing them, you should leave these lists in the document. Comment them out to remove them.
	\listoffigures
	\listoftables
    \printnomenclature %Creates a list of abbreviations. Comment out to remove it. 

% sample text for abbreviations:
NLP is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.
 
\nomenclature{NLP}{Natural Language Processing}
 
$\sigma$ is the eighteenth letter of the Greek alphabet, and carries the 's' sound. In the system of Greek numerals, it has a value of 200. 
 
\nomenclature{$\sigma$}{The total mass of angels per unit area}

% The following sets up the document for the main part of the thesis or dissertation. Do not comment out or remove this line.
	\mainmatter

	%now go ahead and start writing your thesis
	\chapter{Introduction} \label{ch:introduction}

    \chapter{Background} \label{ch:Background}
    The \monitor framework assists Trusted Execution Environment (TEE) based applications running on a non-SGX node to transparently offload their confidential computing tasks to remote nodes with SGX capabilities. \

    The background information necessary to understand \monitor are Trusted Execution Environment (Section~\ref{ase:TEE}), Intel SGX (Section ~\ref{ase:Intel SGX}), Open Enclave (Section~\ref{ss:Openenclave SGX SDK}), CRIU (Section~\ref{ss:CRIU}), Ptrace (Section~\ref{ss:Ptrace})

    \section{Trusted Execution Environment} \label{ase:TEE}
    The TEE provides an isolated environment where the code and the data is guaranteed to be protected with authenticity, integrity and confidentiality [DteeRef]. The prevention is typically achieved through hardware isolation and is active during both the stale and the running state of a program. Any unauthorized entity including the operating system (OS), Hypervisor or the hardware devices will not be allowed to manipulate the code or data residing in the isolated space provided by the TEE. Along with above mentioned features, The TEE provides a feature called Remote attestation to prove its trust worthiness to third parties. Several hardware vendors implement their own flavour of TEE (e.g., Intel SGX [ ], ARM Trust-Zone [ ], AMD SEV [ ], RISC-V Keystone [ ]). In this thesis, we will be focusing on offloading the TEE from Intel, which is Intel SGX.

    \section{Intel SGX} \label{ase:Intel SGX}
    INTEL SGX stands for Intel's Software Guard extensions and it includes a set of extensions that are required in making the TEE possible. It is a successor to technologies such as TPM [] and TXT []. The user loads their code and data into a secured space created by the SGX and it protects the data and code while the computations being performed. Furthermore, the isolated and the secured space in the world of SGX are called as enclaves.

    \subsection{Software Attestation} \label{ss: Software Attestation}
    The primary foundation to SGX's working is Software Attestation, a feature that provides the authenticity of the code being loaded into TEE. In SGX, Software Attestation work as follows. When a user want to run their code into the enclaves, the SGX calculates the hash of the loaded code and data inside it. This hash of the enclave's content is cryptographically signed by the SGX with an attestation key []. This hash value can be verified by the user of the enclave with the public part of attestation key and if the hash value mismatches, the user can refuse to utilize the enclave anymore else if matches the user can start using it. 
    

    \subsection{Working of SGX} \label{ss: Working of SGX}
    The SGX supported CPUs have their own private memory called Processor Reserved Memory (PRM), a memory whose access is protected from kernel, hypervisor, or from any other entities by the CPU. This PRM holds Enclave Page Cache (EPC) to store all the code \& data of an enclave and the state along with the ownership of EPCs to individual enclaves are managed by CPU with the help of Enclave Page Cache Metadata (EPCM). \ 

    It is assumed that untrusted entities, such as the Operating System, possess knowledge of the enclave's initial states. The enclave initialization commences with the OS loading all code and data into the unprotected memory (outside of PRM) [ ], and then requesting the SGX- enabled CPU to transfer the contents into the PRM. During the transfer process, the CPU ensures that the content are cryptographically hashed. Once transfer is complete, the cryptographic hash becomes the enclave's measurement hash. Subsequently, the CPU's acknowledges the OS that the enclave has been initialized and thereby allowing the application software to attest and run the code inside the enclave. \

    Throughout the lifespan of the enclave, multiple mechanisms contribute to the enhancement of enclave security. The application enters into enclave in a fashion similar to switching from user-space to kernel-space along with address translation and an execution at a protected ring level 3. Handling of interrupts while enclave execution results in Asynchronous Enclave Exit (AEX) with CPU saving the state of the enclave in a predefined area of enclave and jumping back into the enclave execution post servicing the interrupt. Exhaustion in the PRM results in OS requesting CPU for EPC eviction. While eviction, the EPC pages are cryptographically hashed by the CPU before they are moved from the protected EPC to unprotected memory.   

    \subsection{Intel SGX SDK} \label{ase:Intel SGX SDK}
    The Intel SGX SDK [] consists of tools, APIs and libraries that allows software developers to use Intel's SGX technology in their applications. The developers of the SDK, can differentiate the enclave functions, referred to as ecalls and the untrusted code, referred to as ocalls with a .edl (enclave descriptor language) file. This .edl file will be parsed by tools such as sgx\_edger8r to place code and define interfaces between the untrusted and the trusted world.

    \section{Open Enclave and its SDK} \label{ss:Openenclave SGX SDK}
    Open Enclave is a project aimed at unifying the isolation primitives of different architectures. Software developers can use Open Enclave SDK [] to develop a single version of their application that could be deployed to Intel's SGX or to a isolation primitive of a completely different architecture such as ARM TEE. The interfacing of Open Enclave's SDK operates in the same way as Intel SGX's SDK. In our experiments, we have also used Open Enclave based applications.

    In Figure~\ref{fig:OE config}, we can see that the application is configured with one function i.e. enclave\_helloworld on the trusted side and an another function i.e. host\_helloworld on the untrusted side. 

    \begin{figure}
    \begin{tcolorbox}
    \begin{verbatim}
    enclave {
        from "openenclave/edl/syscall.edl" import *;
        from "platform.edl" import *;

        trusted {
            public void enclave_helloworld();
        };

        untrusted {
            void host_helloworld();
        };
    };
    \end{verbatim}
    \end{tcolorbox}
    \caption{Open Enclave configuration file for splitting trusted and untrusted code}
    \label{fig:OE config}
    \end{figure}

    \section{Checkpoint/Restore In Userspace} \label{ss:CRIU}
    Checkpoint/Restore in userspace [], shortly CRIU is a linux based application, that allows one to store the state of an application or a container to the disk and restore it with the same state on a same or different node at any time. During checkpointing, the CRIU uses PTRACE APIs to capture information of an individual threads in a process such as Address space map, resources owned (file descriptors, timers and others), register states and many other meta data. These information will be stored as an image file with Google's protobuf [] format in the disk. Upon restoration, the CRIU spawns a child process and restores the process including all of its thread using the information stored within the protobuf formatted image files. 

    \subsection{Compel} \label{ase: Compel}
    The "compel" utility, part of the CRIU project, enables the injection of custom code into a foreign process's context. Its functionality closely resembles injecting code into an unoccupied segment of a process's program memory using PTRACE's "poke text" feature. However, what sets "compel" apart is its sophisticated framework, which facilitates the execution of complex code within the foreign process's context. The compel calls this injected code as "parasite code" as it depends on foreign context to be alive. 

    The source code of the parasite code can be compiled similarly to any other source code, but with the inclusion of compel-specific flags. Subsequently, it can be linked to the infecting source using the "hgen" tool (Header Generator) that comes bundled with "compel." As the name suggests, the "hgen" tool converts this source code into a header file, which must then be included in the source code where you intend to utilize the compel APIs for code injection.

    To execute the parasitic code within the victim task, a specific sequence of steps should be followed. First, one must halt the victim task, then inject the code into it, proceed to execute the necessary code, subsequently remove the previously injected code, and finally, resume the task. Compel provides APIs to perform all of these actions. While injecting code into a foreign process, Compel establishes a socket connection between the infector and the victim, creating a means for commands to be executed from the infector. 

     \section{Ptrace} \label{ss:Ptrace}
     The "ptrace" system call is a mechanism that allows one process to trace and modify the program state of another process, including its memory and register state. In the context of "ptrace," the process performing the tracing is referred to as the "tracer," while the process being monitored and traced is known as the "tracee.". A tracer process can establish a connection with the tracee in two main ways: during the tracee's forking process using "PTRACE\_TRACEME," or at any point in time using "PTRACE\_ATTACH" or "PTRACE\_SEIZE." Once attached, the tracer process will receive notifications through its next "waitpid()" call, which will include information about the signal that caused the tracee to pause or stop. Subsequently, the tracer has the ability to resume the tracee, while also implementing any necessary modifications as intended.

     \section{Userfaultfd} \label{ss:Userfaultfd}
     Linux allocates memory to processes by partitioning physical memory into logical units known as "pages." The standard page size is typically 4KB, but there are various page types of different sizes available, including 2MB "huge pages." When a process is created in Linux, it doesn't automatically receive all the memory pages it may eventually need; it is granted only the pages it requires immediately. Linux, in conjunction with the Memory Management Unit (MMU), manages the mapping from the physical address space to the virtual address space. As the CPU executes machine code instructions one by one, the MMU plays a critical role. If an instruction requires a page that is not currently in memory, the MMU triggers an exception. The MMU determines this by inspecting the page table entries established by Linux, which serve as a map between virtual addresses used by the process and the corresponding physical memory locations. When a needed page is missing from memory, the MMU alerts the CPU, and Linux kernel takes the necessary steps to load the required page into memory.

     The previously mentioned fault handling with the help of 'userfaultfd' \cite{userfaultfd} can also be managed by a dedicated thread within a process. In this scenario, the fault handling thread can create a 'userfaultfd' object to monitor a specific portion of memory utilized by other threads within the same process. Furthermore, it's possible for the 'userfaultfd' to be transferred through socket or any of the IPC mechanism provided by the OS and managed by a thread that belongs to an entirely different process. During the fault handling process, the thread that encounters the fault will be temporarily suspended by the kernel, allowing it to be resumed by the thread responsible for handling the page fault in the user space.
     
    % \begin{table}[htbp]
    % \centering
    % \begin{tabular}{|l|p{0.7\linewidth}|}
    %     \hline
    %     \textbf{Constant} & \textbf{Description} \\
    %     \hline
    %     UFFDIO\_REGISTER\_MODE\_MISSING & Page-fault when a missing page is accessed. \\
    %     \hline
    %     UFFDIO\_REGISTER\_MODE\_MINOR & Page-fault when the backing page is in the page cache. \\
    %     \hline
    %     UFFDIO\_REGISTER\_MODE\_WP & Page-fault when the write-protected page is written. \\
    %     \hline
    % \end{tabular}
    % \caption{UFFDIO\_REGISTER\_MODE Constants}
    % \label{tab:uffdio-modes}
    % \end{table}
    
     In general, the page-fault handling thread will use the UFFDIO\_REGISTER\_MODE\_WP feature of Linux to write-protect the page. This write protection mechanism is employed to notify the fault handling thread when the fault-incurring thread attempts to write to the page. Table lists the modes available in userfaultfd for Linux.
     
    \chapter{Related Works} \label{ch:rel_works}
    This chapter provides an overview of works related to \monitor. Section \ref{ase:frameworks_to_improve_usability} delves into related works focussing on enhancing the usability of hardware enclaves. Following that, Section \ref{ase: Runtime_migrations} explores works in the realm of runtime migrations.

    \section{Frameworks improving the usability of Enclave} \label{ase:frameworks_to_improve_usability}
    There exists a substantial body of work that aims to enhance the usability of enclave technology on diverse platforms such as Intel's SGX \cite{Intel-SGX}, AMD's SEV \cite{AMD-SEV}, IBM's SecureBlue++ \cite{IBM-Secure_Blue}, ARM's TrustZone \cite{ARM-TrustZone}, and numerous others. The principle design choice behind the framework employing an enclave usually falls into two categories, one is based out of Library OS (\ref{ase:LibOS based Frameworks}) or the other one is based out of thin "shim" layer (\ref{ase:Shim Layer based Frameworks}). 

    \subsection{LibOS based Frameworks} \label{ase:LibOS based Frameworks}
    The Library OS is nothing but an Unikernel \cite{Unikernel} where the entire program is compiled along with the operating system code that the program depends. This way, users leveraging such frameworks can defeat attacks such as "lago attacks" \cite{lago}, where a malicious OS could subvert a protected application by manipulating the results of system calls. Although this approach has received criticism in terms of increased service latency, decreased service throughput, and an enlarged Trusted Computing Base (TCB), many frameworks continue to leverage Library OS with a promise to provide a reduced service latency and improved throughput. 

    For instance, The Gramine (formerly known as Graphene) \cite{Gramine} is Library OS that allows an unmodified Linux binary to be executed on top the SGX protected environment along with a number of enhanced functionality such as secured-interprocess-communication, integrity of dynamically loaded libraries and many others. To use Gramine, users are required to provide a manifest file that specifies resource access rules, including file system access, dynamic libraries to be loaded, and the integrity measurements of the components to be used, as well as any other forms of access such as network permissions for the applications. All of the above mentioned accesses of the application are monitored by the reference monitor on the fly. Furthermore, while the enclave is being loaded, the initial state of the enclave is measured and then attested by the CPU along with the hash of the manifest file, thereby ensuring the integrity of all the dynamic libs needed by the application. 

    Similarly, Haven \cite{Haven} is an another framework that is based out of Library OS design principle. It is derived out of a Library OS called "Drawbridge" \cite{Drawbridge} to implement a mutually distrusting Windows 8 API using a set of primitives such as threads, memory and others. It facilitates the execution of unmodified Windows applications within the secured environment of Intel SGX.  

    \subsection{Shim Layer based Frameworks} \label{ase:Shim Layer based Frameworks}
    The "shim" layer \cite{shim}, in contrast, intercepts API calls for the purpose of modifying and redirecting them to achieve specific outcomes. In the case of frameworks that create the environment for enclave execution, this involves altering the enclave's external interface such as system calls to protect against potential threats from the underlying host OS or hypervisors.

    Secure Container Environment (SCONE) \cite{SCONE}, is a framework that secures all the processes running inside the container and the container itself by executing them altogether inside the Intel SGX's protected mode. SCONE is based on the 'shim' layer principle, where an external interface to the host OS is created at the level of system calls executed by the libc implementation residing within the enclaves. SCONE refers to its shim layer as a 'shield' and it is currently available for the file system, network, and console. This shield is designed to prevent low-level attacks, such as those from the host OS that may attempt to manipulate the pointers and buffer sizes passed to applications. Additionally, it ensures the integrity and confidentiality of the application's data as it passes through the OS.

    Similar to SCONE, PANOPLY \cite{PANOPLY} offers POSIX abstractions for accessing filesystems, networks, multi-processing, threading, and other fundamental features to the application logic. These frameworks, which are built on the principles of the shim logic, assert smaller Trusted Computing Base (TCB) sizes and lower performance overhead.  Additionally, frameworks like Asylo \cite{Asylo} and Open Enclave \cite{Open-Enclave}, Intel SGX \cite{Intel-SGX} exhibit similar behavior.

    \subsection{Difference of \monitor over LibOS and Shim based frameworks}
    In contrast to the aforementioned frameworks, which facilitate in providing easier accessibility to enclave-based environments on devices equipped with SGX capabilities, \monitor goal is different and its goal is to offer enclaves for devices that lack SGX support by employing Enclave offloading to remote SGX based devices through frameworks such as Open Enclave \cite{Open-Enclave} \& Intel SGX SDK \cite{Intel-SGX}. 

    \section{Runtime migration of applications among Homogeneous \& Heterogeneous-ISA Systems} \label{ase: Runtime_migrations}
    Runtime migration between nodes stands as a prominent theme in the field of distributed systems, with diverse practical implications. The applications include ensuring fault tolerance, enabling disaster recovery in case of failures, facilitating mobility in edge computing for faster data processing, supporting dynamic scaling, optimizing energy efficiency for load balancing, and many other applications \cite{CRIUMigration}. These migrations can occur among homogeneous architectures, such as from x86 to x86, or among heterogeneous architectures, for example, from x86 to aarch64.

    \subsection{Homogeneous migrations}
    Checkpoint/restore (CRIU) \cite{CRIU}, for instance is a project to implement a homogeneous migration of a process in Linux environment from one node to an another. It works by suspending the state of an application (or a container) and saving the state of the process to an image file in the disk with a protobuf format \cite{Protobuf}. These image files store the program's state and can subsequently be used to restore the application (or a container) on the desired node, provided that it has the same architecture, at any required time. They also offer support for lazy migrations, a method in which data is transmitted to the remote node via sockets only when it is required at that specific moment, rather than transferring all the data upfront. Lazy migration enhances efficiency by reducing unnecessary data transfer and conserving system resources, ensuring that data is delivered precisely when it is needed. Besides CRIU, there are several other frameworks that deploy relocation of the program state such as \cite{migration:wiki} 

    In contrast, the \monitor for homogeneous offloading, serves as a lightweight monitoring process that effectively offloads the processing of its child process's enclave function to a remote SGX node by synchronizing the needed memory whenever the enclave function is to be executed on the remote node.

    \subsection{Heterogeneous migrations}
    In the realm of cloud computing, there has been a noticeable increase in the adoption of heterogeneous Instruction Set Architectures (ISAs). This trend is primarily driven by the promise of improved performance and enhanced power efficiency for a wide range of applications. Heterogeneous ISAs along with runtime migration combine the benefits of both. 
    
    Several works such as code migration \cite{hcontainer, het_isa_migrate:asplos12}, code offloading \cite{offload:mobisys17} and dynamic binary translation (DBT) \cite{dbt:tc01} aid applications to run on CPUs of heterogeneous-ISAs. The Dynamic binary translation converts a short code sequence (i.e. basic blocks) of a source instruction set to a destination instruction set for cross-architectural migration. In \cite{offload:mobisys17}, Wang et-al leverages the DBT to offload the computations to server for energy efficiency of mobile devices. 

    Popcorn Linux \cite{het_isa_migrate:asplos12} prepares applications for cross-architecture migration by generating multi-ISA binaries. Each of the generated binary will contain the modified code and data sections suitable for migration across architectures. The code and data section of these binaries are laid out in a common format so that an uniform virtual address space layout appears across the modified Popcorn Linux kernel. Additionally, these binaries contain metadata that can be leveraged by the embedded runtime state engine, enabling state transformations within the application.  

    HeterSec \cite{HeterSec} is an another work that aims to secure process execution by utilizing ISA heterogeneity. It is based out of distributed kernel system where each kernel runs on a heterogeneous ISA multi-domain "machine" connected by a high speed infiniband connection. On a high level, HeterSec has three major functionality, per-process page table synchronization, system resource sharing service and securing the applications. The pages and the system resources are shared and synchronized with the help of a dedicated kernel threads. 
    
    On the other hand, \monitor operates more like a Remote Procedure Call (RPC)-based approach for offloading tasks in heterogeneous environments. It is specifically designed to work with applications that are built on the Open Enclave SDK. With \monitor, the wrapper code from the Open Enclave framework is modified so that the enclave function requests are forwarded to a node that supports SGX for execution.

    \chapter{Design} \label{ch:design}
    The \monitor aims to transparently offload confidential computations to a computing node with SGX support. To achieve this, the \monitor needs to transparently split the execution of code \& data into multiple nodes and synchronize the computations from one node to the other while switching the executing node. 

    The Design chapter is divided into the following sections. Section \ref{ase:monitor design} outlines the requirements for two distinct modes of the \monitor, namely the "Monitor" mode and the "RPC" mode. Section \ref{ase: Monitor mode} delves into the comprehensive design of the "Monitor" mode of the \monitor. In a parallel fashion to Section \ref{ase: Monitor mode}, Section \ref{ase: RPC mode} elaborates on the overall design of the "RPC" mode.

    \section{Different modes of \monitor} \label{ase:monitor design}
    The sole purpose of \monitor is to facilitate non-SGX nodes with SGX capability. These non-SGX nodes can be based on various architectures, such as aarch64, x86, or others.  However, it's important to note that the SGX node can only be based on Intel's x86 architecture, as SGX is a Trusted Execution Environment (TEE) exclusive to Intel. Software Development Kit (SDK) such as Intel SGX SDK \cite{Intel-SGX} provides tools, libraries and APIs that allow developers to build applications that execute within the SGX as a TEE on x86 architecture. On the other hand, the Open Enclave SDK \cite{Open-Enclave} takes a more versatile approach. It supports a variety of TEE architectures by abstracting the differences between different enclave types and creating a universal programming interface. To enable enclave offloading to the SGX node using either of these SDKs, the \monitor operates in two distinct modes: 'Monitor' mode for homogeneous (i.e. between x86 non-SGX and x86 SGX) and 'RPC' mode for heterogeneous (i.e. between x86 or aarch64 non-SGX and x86 SGX). 

    \begin{figure}[htb]
	    \centering
		\includegraphics[scale=1.3]{figures/Arch_support_sgx2.png}
		\caption{Architecture supported by different modes of \monitor} 
		\label{fig:arch_supported}
	\end{figure}

    As depicted in the Figure. \ref{fig:arch_supported}, the 'Monitor' mode is applicable to the  x86 non-SGX nodes using Intel SGX SDK and the Open Enclave SDK. On the other hand, the 'RPC' mode can be employed for both x86 and aarch64 based non-SGX nodes when using the Open Enclave SDK. 

    Before looking into the design, let us familiar with some of the common terminologies used.

    \begin{enumerate}
        \item Client-node: This is a non-SGX node that requires SGX functionality. The SGX-based user application is initially launched on this node, and the execution context is transitioned whenever an enclave function needs to be executed.
        \item Server-node: This is the SGX node responsible for facilitating SGX functionality. It remains in an idle state until a request comes from the client node.
        \item \monitor monitor: The monitor process is a Ptrace-based tool that suspends and resumes the user's SGX-based application while transitioning between nodes. Additionally, it manages inter-node communication across nodes for enclave offloading. 
    \end{enumerate}
    
    Let us now discuss the two distinct modes of \monitor one by one in the following sections. 

    \section{Monitor mode of \monitor for Homogeneous migrations} \label{ase: Monitor mode}
    As previously discussed in Section \ref{ase:Intel SGX SDK}, the Intel SGX SDK incorporates an Enclave Description Language (EDL) file to establish interfaces between the untrusted and trusted components of applications. This SDK offers support for both "Call by value" and "Call by reference" argument types for ecalls and ocalls functions. Within the SDK, the sgx\_edger8r tool plays a critical role by generating wrappers to facilitate interactions with the underlying SGX driver. However, a noteworthy point is that in these wrappers, the arguments provided by the user functions, whether passed by value or by reference, are directly forwarded to the underlying SGX interface. As a result, to ensure the smooth transition of application execution between nodes, the \monitor must maintain memory consistency across nodes. This becomes particularly critical when dealing with function calls that involve memory pointers. The preservation of memory coherence is essential to guarantee the correct and expected behavior of the application. This maintenance of memory consistency is a key aspect of the \monitor's "Monitor" mode.

    \subsection{High level design of "Monitor" mode} \label{ase: highlevel_design_monitor}
    The "Monitor" mode of the \monitor operates as follows and the same has been depicted in the Figure.\ref{fig:MonitorArch} as well:

\afterpage{
    \begin{figure}[H]
    \centering
	\rotatebox{270}{\includegraphics[scale=1.2, height=0.7\textwidth]{figures/monitor_arch_mini.jpg}}
	\caption{Overall Architecture of HARES's Monitor mode} 
	\label{fig:MonitorArch}
	\end{figure}
}

    \begin{enumerate}
        \item As a starting point, the user must provide the enclave call instructions and their corresponding return instructions to the \monitor framework on both the server and the client nodes. 
        \item The behavior of both the client-side and server-side \monitor will differ. Initially, the client-side \monitor will set breakpoints for all the user-provided enclave instructions before launching the user's application. In contrast, the server-side \monitor will pause the application and await a request from the client side.
        \item The client \monitor, upon capturing the execution of the enclave call triggered by the breakpoint, will then proceed to request the server \monitor to execute the enclave function.
        \item The server \monitor will initiate memory and register state synchronization as the initial step before proceeding to execute the enclave call.
        \item Upon successful execution of the enclave call, the execution flow will return to the client-side \monitor. The client \monitor will then follow the same steps as the server \monitor, including memory and register state synchronization, and continue execution until the next breakpoint is encountered.
        \item These above steps from (3) to (5) will be repeated whenever the client \monitor encounters an enclave call execution.
    \end{enumerate}

    \subsection{Memory Synchronization by \monitor}
    In order for the \monitor to offload the enclave functions, which may involve both "Call by value" and "Call by reference" arguments, it is essential to perform memory synchronization each time the execution transitions between different nodes, whether on the server or client side. 
    
    Memory synchronization is indeed a resource-intensive and time consuming process, demanding the maintenance of a consistent view of numerous pages across various Virtual Memory Area (VMA) regions, which can be quite extensive in size. In the Linux kernel, VMAs are used to track a process's memory mappings. Each process has separate VMA regions for different types of memory, including Stack, Heap, File-backed, and others. Each VMA is characterized by a starting address, size, permissions, and control flags. Some VMAs, like stack and heap, exhibit dynamic changes in size, expanding or shrinking, while other VMA types, such as file-backed, may be created or deleted during the course of a process's lifetime.

    The \monitor must ensure that the most recent state of these VMAs remains consistent across the nodes, including both size and content. To extend the size of a specific region, the brk system call \cite{systemcall::brk} can be employed, and similarly, to create new regions, the mmap system call \cite{systemcall::mmap} is used. The contents can be updated with the help of calls such as process\_vm\_readv \cite{processVmReadv} and process\_vm\_writev \cite{processVmWritev}. Importantly, all of these calls must be executed from the context of the user application, and this is where the application of compel's parasitic injection comes into play. This is depicted in Figure \ref{fig:vmas_incremental}. Initially, there are five VMA regions. After the Ecall execution on the remote, the extended Heap (highlighted in green) is propagated to the Client node (marked in orange). Before offloading another Ecall function, the client generates a new VMA region of the File-backed type, which is then sent to the Remote. This process continues until the application completes execution.

    \begin{figure}[htb]
	    \centering
		\includegraphics[scale=0.7]{figures/incremental_vmas.png}
		\caption{Updates of VMA in an incremental fashion} 
		\label{fig:vmas_incremental}
	\end{figure}

    \subsection{Faster memory synchronization \& its need in \monitor}
    An application served by the \monitor may involve numerous enclave function calls, necessitating frequent switches in execution context to a different node. During each of these execution context switches, transferring the entire process memory from one node to another is highly inefficient, and it can result in significant latency. Therefore, the memory synchronization technique chosen by the \monitor framework should prioritize efficiency and speed. Minimizing any performance degradation for users is essential, and a robust, optimized memory synchronization mechanism can significantly enhance the overall efficiency and responsiveness of the framework.

    One way to optimize memory synchronization is by transferring only the pages that have changed since the resumption of execution context on this node until the execution context switches to a different node. This ensures that the pages are synchronized across both the server and client sides. The Figure.\ref{fig:pages_incremental} depicts the optimized memory synchronization. Initially, two new pages are created (highlighted in green) and sent to the server side (marked in yellow). Following this, an Ecall is executed, resulting in the generation of another page (depicted in green), and this process continues.

    \begin{figure}[htb]
	    \centering
		\includegraphics[scale=0.7]{figures/incremental_pages.png}
		\caption{Updates of pages in an incremental fashion} 
		\label{fig:pages_incremental}
	\end{figure}


    In order to identify the pages that have been altered, the \monitor employs a separate dedicated thread for handling page faults in the user space using 'userfaultfd,' as mentioned in Section \ref{ss:Userfaultfd}. The \monitor will write-protect all pages every time code resumes in a particular node. The pages that experience faults are identified as modified pages.

    \section{RPC mode of \monitor for Heterogeneous migrations} \label{ase: RPC mode}
    The Open Enclave SDK supports Trusted Execution Environments (TEE) for both x86 and aarch64 platforms. Aarch64 applications can leverage Arm's TrustZone technology, while x86 applications can utilize Intel's SGX (Software Guard Extensions). However, it's essential to note that applications compiled for one architecture (e.g., Aarch64) cannot use the TEE of the other architecture (e.g., Intel's SGX), and vice versa. This is the precise challenge that the \monitor aims to address. 

    The \monitor addresses the challenge of executing enclave functions across different architectures (Aarch64 and x86) through the use of Remote Procedure Calls (RPC) \cite{Remote-procedure-call}. Remote Procedure Calls allow procedures to be executed in the address space of a remote process, as if they were executed in the same address space. This remote process can be located either locally or remotely.

    By converting the enclave functions called from Aarch64 into RPCs, the \monitor enables these enclave functions to be executed across platforms, allowing them to run within Intel's SGX on the x86 platform. This approach helps bridge the architectural gap and facilitates the execution of enclave functions across different platforms. 

    \subsection{Serialization of Arguments for RPC by Open Enclave}
    To enable the enclave function to function as an RPC effectively, it is crucial that all arguments, whether passed by "Call by Value" or "Call by Reference," undergo serialization before they are transmitted across various platforms.

    \begin{figure}
    \begin{tcolorbox}
    \begin{verbatim}
trusted {
    public int seal_data(int sealPolicy,
                [in, size = opt_msg_len] unsigned char* opt_mgs,
                size_t opt_msg_len,
                [in, size = data_size] unsigned char* data,
                size_t data_size,
                [out] data_t* sealed_data);

    public int unseal_data([in] const data_t* sealed_data,
                const int optional_msg_flag,
                [out] data_t* output_data);
};
    \end{verbatim}
    \end{tcolorbox}
    \caption{Open Enclave configuration file for Data Sealing application}
    \label{fig:Data Sealing oe config}
    \end{figure}
    
    The auto-generated wrapper code provided by the Open Enclave SDK already performs
    some level of argument serialization before invoking the enclave function. Before we proceed further, let’s examine what this serialization process entails and how it contributes to the functioning of enclave functions in an RPC context.

    Consider the Figure \ref{fig:Data Sealing oe config}, which represents the EDL (Enclave Description Language) configuration file for the Data-Sealing application, our focus will be on the seal\_data function. This function takes five arguments, of which the first four arguments are provided by the user, while the last argument "sealed\_data" is populated by the enclave function post execution. It is noteworthy that the arguments "opt\_msg," "data," and "sealed\_data" are passed by reference.

    \begin{figure}
    \begin{tcolorbox}
    \begin{verbatim}
    /**** ECALL marshalling structs. ****/
    typedef struct _seal_data_args_t
    {
        oe_result_t oe_result;
        uint8_t* deepcopy_out_buffer;
        size_t deepcopy_out_buffer_size;
        int oe_retval;
        int sealPolicy;
        unsigned char* opt_mgs;
        size_t opt_msg_len;
        unsigned char* data;
        size_t data_size;
        data_t* sealed_data;
    } seal_data_args_t;
    \end{verbatim}
    \end{tcolorbox}
    \caption{Marshalling struct auto-generated by Open Enclave}
    \label{fig:Marshalling struct}
    \end{figure}


    The ECALL marshalling structure, depicted in Figure \ref{fig:Marshalling struct}, is an auto-generated structure created by Open Enclave for passing arguments into enclave calls. Before passing this structure to the ECALLs, its content needs to be marshaled and serialized. The method employed by Open Enclave for this purpose is illustrated in Figure \ref{fig:OE-Serialization}.

\afterpage{
    \begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[scale=0.4]{figures/rpc_buffer_before_serialization.png}
        \caption{Open Enclave's buffer before serialization}
        \label{sfig:rpc_buffer_before_serialization}
    \end{subfigure}
    \\[30pt]
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[scale=0.4]{figures/rpc_buffer_after_serialization_oe.png}
        \caption{Open Enclave's buffer after serialization}
        \label{sfig:rpc_buffer_after_serialization_oe}
    \end{subfigure}

    \caption{Serialization by Open Enclave}
    \label{fig:OE-Serialization}
    \end{figure}
}

    At first, Open Enclave allocates a buffer in the heap with sufficient size to accommodate not only the marshalling structure holding the function's arguments but also with                the size of contents that could be dereferenced by the pointers in the arguments. As seen in Figure \ref{sfig:rpc_buffer_before_serialization}, the 'OE Input Buffer before serializing' holds the marshalling structure at address 0xD000 and the content to be dereferenced at 0xD100. At this point, the pointers within the marshalling structure still reference the memory addresses initially created by the user. For instance, 'opt\_msg' is at 0xA000, while 'data' is at 0xFF00. As part of serialization, all contents pointed to by the user-given arguments will be placed within the Open Enclave-generated intermediary buffer, and the references within the marshalling structure will be changed to point within this intermediary buffer. For example in Figure.\ref{sfig:rpc_buffer_after_serialization_oe}, the 'opt\_msg' pointer now points to 0xD100, the 'data' pointer points to 0xD200, and so on. If there are any nested pointers, their contents will also be copied, and their references will be adjusted accordingly

    \subsection{Pointer adjustment \& readjustment by \monitor} \label{adj_and_readj_of_monitor}
    \afterpage{
    \begin{figure}[H]
        \centering
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[scale=0.4]{figures/rpc_buffer_hares_serialization.png}
            \caption{Open Enclave's buffer after HARES adjustment in client}
            \label{sfig:rpc_buffer_hares_serialization}
        \end{subfigure}
        \\[30pt]
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[scale=0.4]{figures/rpc_buffer_hares_before_readjustment.png}
            \caption{Open Enclave's buffer with HARES before readjustment in server}
            \label{sfig:rpc_buffer_hares_before_readjustment}
        \end{subfigure}
        \\[30pt]
        \begin{subfigure}{\textwidth}
            \centering
            \includegraphics[scale=0.4]{figures/rpc_buffer_hares_after_readjustment.png}
            \caption{Open Enclave's buffer HARES readjustment in server}
            \label{sfig:rpc_buffer_hares_after_readjustment}
        \end{subfigure}
        \\[20pt]
        \caption{Open Enclave's buffer manipulation by HARES}
        \label{fig:HARES-Serialization}
    \end{figure}
}

    The references made by the pointers within the marshalling structure, however, lose their meaning when passed to the remote node. Hence, an adjustment of pointers needs to be made by the \monitor before transferring. Figure \ref{fig:HARES-Serialization} illustrates this process. Pointers within the marshalling structure are assigned addresses relative to the buffer's starting address (0xD000 in this case). After the \monitor's adjustment from Figure.\ref{sfig:rpc_buffer_hares_serialization}, we observe that pointers such as 'opt\_msg' have a value of 0x0100, 'data' has 0x0200, and so on.
    
    This serialized and adjusted buffer is then passed from the Client node to the Server node, where a readjustment takes place. A new temporary buffer of the exact size is created, holding the data passed from the Client node on the Server side. Referring to Figure \ref{sfig:rpc_buffer_hares_before_readjustment}, the newly created buffer is at the address 0x1000, with the pointers still holding the adjusted relative address values such as 0x0100, 0x0200, and so on. These adjusted values are then further readjusted to point within this temporary buffer, ensuring that subsequent dereferences pose no issues and remain consistent with memory access. The pointers, after readjustment, will have values such as 0x1100, 0x1200, and so on. For nested pointers, the adjustment and re-adjustment have to be made in a similar fashion. The same could be employed for the ouptut arguments such as the argument pointer sealed\_data from the seal\_data function from the Figure. \ref{fig:Data Sealing oe config}

    \chapter{Implementation}
    The \monitor was developed to extend SGX capabilities to devices that lack native SGX support, effectively enabling SGX offloading from non-SGX x86 or aarch64 devices. It provides two distinct modes: "Monitor" and "RPC."

    This chapter delves into the intricacies of the \monitor's design, exploring both of its operational modes. Section \ref{ase:implementation of monitor mode} and its corresponding subsections delve into the implementation details of the monitor mode, while Section \ref{ase:implementation of rpc mode} elaborates on the intricacies of the RPC mode's implementation. 
    
    \section{Implementation of Monitor mode} \label{ase:implementation of monitor mode}
    The monitor mode provides a design that is internally framework-agnostic. It is accompanied by a "monitor" process that manages the transition from one node to another while simultaneously ensuring memory synchronization. The modules used by the \monitor are discussed in the following subsections.
    
    \subsection{Monitor's Configuration} \label{ase: Json Parser}
    
    \begin{figure}[htb]
        \centering
        \includegraphics[scale=2.0]{figures/transisitonal_assembly_inst.png}
        \caption{Assembly instructions calling and returning from Enclave functions.} 
        \label{fig:transition_assembly}
    \end{figure}
 
    The 'Monitor' mode of the \monitor enables the transition of the execution context to a different node at a specific instruction. This instruction may involve calling an Enclave function, such as 0x40aaef in Figure \ref{fig:transition_assembly} (Transition from client to server), or returning from the Enclave function, like 0x40aaf4 in Figure \ref{fig:transition_assembly} (Transition from server to client). To facilitate this transition, users of the \monitor need to make it aware of these transitional instructions, which is why the \monitor requires configuration. The configuration file is in JSON format, and an example configuration file used for the HelloWorld application is illustrated in Figure \ref{fig:config_monitor}, with the fields listed as follows:
    
    \begin{itemize}
        \item \texttt{main\_address}: Gives the address of the main function.
        \item \texttt{user\_args}: Provides the user arguments for starting the \monitor's client application, which is the user application itself.
        \item \texttt{breakpoints}: Specifies the transitional instruction addresses. Odd instructions are considered as calls to Enclave Functions, and even instructions are considered as returns from Enclave functions.
    \end{itemize}
    
    \begin{figure}[htb]
        \centering
        \includegraphics[scale=2.0]{figures/configuration_json.png}
        \caption{JSON-based configuration file of \monitor "Monitor" mode.} 
        \label{fig:config_monitor}
    \end{figure}
    
    Configurations, such as the one in Figure \ref{fig:config_monitor}, will be parsed by the \monitor monitor process to set breakpoints and transition the execution context.

    \subsection{The Compel Handler of \monitor} \label{ase:compel handler}
    As stated in Section \ref{ase:compel handler}, the \monitor uses compel's parasitic injection to steal the userfault-fds and handle the extensions of the VMAs. Whenever the \monitor intends to execute a specific command through the parasitic code, it has to send the command along with the appropriate argument. The commands used by the \monitor are listed in Figure \ref{fig:comple_interface}. The purpose of individual commands is listed below:

    \begin{itemize}
        \item \texttt{PARASITE\_CMD\_GET\_STDUFLT\_FD} is a command used for the registration of the userfault-fd. The arguments used are the pointer where the fd is to be filled, the starting address, and the number of pages for uffd registration.
        \item \texttt{PARASITE\_CORRECT\_HEAP\_OFFSET} is a command used to extend the size of the heap. The argument that will be passed is the number of pages to extend.
        \item \texttt{PARASITE\_CMD\_REM\_STDUFLT\_FD} is a command used to remove the userfault-fd registration. The arguments are the file descriptor, the starting address, and the number of pages for de-registration of uffd.
        \item \texttt{PARASITE\_CMD\_CREATE\_MMAP} is a command used to create a new MMAP-ed region. The arguments used are the desired address and the number of pages to be allocated for this memory region.
    \end{itemize}


    \begin{figure}[htb]
        \centering
        \includegraphics[scale=2.0]{figures/compel-macros.png}
        \caption{Compel's interface commands by HARES.} 
        \label{fig:comple_interface}
    \end{figure}

    Another crucial consideration is that during the registration of the userfaultfd for all the write-permitted regions, we must ensure that we ignore the VMA of the user application that belongs to the parasitic code. Otherwise, the parasitic code would encounter a write fault and wait for the \monitor to handle it through the userfaultfd. However, by then the \monitor is still registering the uffd for other regions of the user application, thus leading  to a deadlock.

    \subsection{Userfault handler thread of \monitor} \label{ase:userfaultfd handler}
    The userfault handler thread is responsible for handling all page faults of the user's application in the user space. All userfault file descriptors (uffds) maintained by the thread will be destroyed and re-registered whenever a new VMA has been created or destroyed, facilitating easy maintenance of uffds. During a transition from a remote node to the current node, all the pages within all VMA regions will be write-protected, as illustrated by the red boxes in Figure. \ref{fig:uffd_handler}, to identify the pages that will be modified. Any writes further down the line until a transition to a remote node will trigger a page fault, and the pages will then be marked as modified, accompanied by write unprotection, similar to the green boxes in Figure. \ref{fig:uffd_handler}.

    \begin{figure}[htb]
        \centering
        \includegraphics[scale=0.7]{figures/uffd_handler.png}
        \caption{UFFD handler handling pages.} 
        \label{fig:uffd_handler}
    \end{figure}

    Furthermore, the userfaultfd does not allow one to register for file-backed pages, and hence, all the pages within the VMA regions belonging to file-backed pages have to be sent every time during the transition. 
    
    \subsection{Memory synchronization APIs used} \label{ase:memsync_api}
    The \monitor framework utilizes inter-node TCP/IP APIs to transfer necessary information for remote execution of the application, such as memory and register state, and to exchange synchronization messages. Optionally, Hares allows establishing an SSL/TLS communication channel to secure the messages. 

    Table \ref{t:TPayload} lists the synchronization APIs used and their corresponding payload sizes. For clarity, The \monitor in the server will be referred to as HARES-s, and similarly, the \monitor in the client will be called HARES-c. Transitions can occur in both directions, with the REMOTE\_EXECUTE header being the first.  When transitioning from Hares-c to Hares-s, the instruction until which execution has to happen is sent as the payload. The Hares-s reads this payload to set a breakpoint in the application and facilitate the transition back to Hares-c. Also, it’s worth noting that when transitioning from the Hares-s to the Hares-c, the payload could be empty because the application in the client will execute as long as it hits the breakpoint set by the Hares-c. The second message header would be
    REMOTE\_REGS, with the payload consisting of the register states themselves. This information is needed to set the register sets of the application properly and execute the required portion of the code.
    
    The next message header is VMA\_FROM\_REMOTE, which indicates the number of VMA regions or pages that will follow in subsequent messages. The message header VMA\_BUFFER \_HEADER then provides additional information, such as the number of pages and the starting address of the application’s memory to follow in the subsequent message. The following message, VMA\_BUFFER, contains the payload of the page itself. Finally, the communication ends with the receiver side sending VMA\_TRANS\_ACK along with acknowledgment for every
    one of the above-mentioned message headers. In Table \ref{t:TPayload}, the largest message is the VMA\_BUFFER message, which is 5004 bytes in size
    because it contains the page itself, which is typically 4096 bytes in size, along with additional information such as the page address.

    \begin{table}[h]
	\centering
	\footnotesize
	\caption{Inter-node TCP/IP messages with its payload size.}
	\begin{tabular}{| c | c |} \hline
		  Messages & Bytes transferred\\ \hline \hline
        REMOTE\_EXECUTE  &  16 \\ \hline
        REMOTE\_EXECUTE\_REPLY  &  8 \\  \hline
        REMOTE\_REGS &  220 \\  \hline
        REMOTE\_REGS\_REPLY & 8 \\ \hline
        VMA\_FROM\_REMOTE &  16 \\ \hline
        VMA\_FROM\_REMOTE\_REPLY &  8\\ \hline
        VMA\_BUFFER\_HEADER & 28 \\ \hline
        VMA\_BUFFER\_HEADER\_ACK & 8 \\ \hline
        VMA\_BUFFER (Per page) & 5004\\ \hline
        VMA\_BUFFER\_ACK (Per Page) & 8 \\ \hline
        VMA\_TRANS\_ACK & 8\\ \hline
	\end{tabular}
	\label{t:TPayload}
    \end{table}

    \subsection{Instruction cleaner} \label{ase:inst_cleaner}
    \begin{figure}[htb]
        \centering
        \includegraphics[scale=0.7]{figures/server_instruction_cleaned.png}
        \caption{Instructions cleaned in the server side.} 
        \label{fig:server_instructions}
    \end{figure}
    
    If we take a look at Figure. \ref{fig:MonitorArch}, the server side and the client side are bloated with instructions that aren't supposed to be executed in their respective ends. For instance, instructions belonging to the enclave wrapper code are not executed on the client side, and similarly, all instructions belonging to codes other than the enclave wrapper are not executed on the server side. Hence, these non-executed instructions are replaced with x86's NOP using a cleaner script before deploying with the \monitor mode. Figure. \ref{fig:server_instructions} \& \ref{fig:client_instructions} show the cleaned assembly instructions where oe\_create\_enclave is the wrapper for the enclave function, and main is the non-wrapper function displayed.

    \begin{figure}[htb]
        \centering
        \includegraphics[scale=0.8]{figures/client_instruction_cleaned.png}
        \caption{Instructions cleaned in the client side.} 
        \label{fig:client_instructions}
    \end{figure}

    \section{Implementation of RPC mode} \label{ase:implementation of rpc mode} 
    Unlike the "Monitor" mode of the \monitor, where HARES-c and HARES-s monitor processes were required on both sides, the RPC mode modifies the wrapper code of the user applications. In this mode, the \monitor modified wrapper code residing on both sides handles all enclave offloading. To modify the wrapper code of the user application, the \monitor leverages and modifies the oeedger8r tool. This oeedger8r tool is an auto code generator used by Open Enclave to generate the wrapper code that sets up a trampoline between the trusted and untrusted worlds. 

    \afterpage{
        \begin{figure}[htb]
            \centering
            \includegraphics[scale=0.80]{figures/rpc_mode_arch.png}
            \caption{\monitor Edger8 tool generating wrapper codes.} 
            \label{fig:hares edger8 tool generating wrapper codes}
        \end{figure}
    }

    \subsection{Server and Client wrapper of \monitor RPC mode} \label{Server and client wrapper of RPC mode}
    
    The \monitor version of the edger8r tool, called \monitor edger8r, develops wrappers in a different way depending on whether the node is SGX capable or incapable. This is illustrated in Figure \ref{fig:hares edger8 tool generating wrapper codes}. On the server side, the wrapper created by the modified tool establishes a closed loop that involves generating and sending the reference of the enclave instance to the requester. It also manages the execution of valid enclave calls with pointer adjustment and readjustment, as discussed in Section \ref{adj_and_readj_of_monitor}. On the client side, all of the enclave function's definitions are replaced with TCP socket-based communication APIs that transmit the necessary information to the server side. Figure \ref{fig:server_state_machine} shows the state machine of the HARES wrapper code within the server side.

    \begin{figure}[htb]
        \includegraphics[scale=0.3]{figures/server_state_machine.png}
        \caption{State machine of the server.} 
        \label{fig:server_state_machine}
    \end{figure}
    
    The server application and the client application in Figure \ref{fig:hares edger8 tool generating wrapper codes} give us a high-level idea of how the RPC mode of \monitor works. Initially, the untrusted part of the app residing in the non-SGX node executes as it is until it wants to execute an enclave function. Once the execution reaches the create enclave function, the wrappers created by the \monitor edger8r send all the information needed to the wrappers residing on the other node, i.e., the SGX capable node. Here, the wrapper code calls the enclave function as it would be normally generated. After enclave execution, the control is transferred back to the non-SGX node with the client-side wrapper created by the \monitor edger8r tool. The above-mentioned logic continues as long as an enclave function is to be executed until the end of application execution. 

    \chapter{Evaluation}
    This chapter is organized into the following sections. Section \ref{ase:experimental setup} introduces the experimental setup used for evaluating \monitor. Following that, Sections \ref{ase:Performance breakdown of remote enclave calls}, \ref{ase:performance evaluation of monitor mode}, \ref{sse:system_call_events_monitor_mode} delve into the performance evaluation of applications, providing a breakdown for the \monitor Monitor mode. Similarly, Sections \ref{sse:Performance evaluation of RPC mode}, \ref{sse:Profiling of serialization and deserialization cost for RPC mode} discuss the performance of \monitor RPC mode. Finally, Section \ref{sse:Security evaluation} conducts the security evaluation of \monitor.
    
    \section{Experimental setup} \label{ase:experimental setup}
    The evaluation setup consists of two machine nodes: an SGX-enabled machine and a non-SGX machine. For the non-SGX machine, we used Neoverse-N1 based Aarch64 machine and an Intel Xeon-based x86 machine for SGX heterogeneous offloading. On the other hand, for homogeneous offloading, we employed two Intel Xeon-based machines, one with SGX enabled and the other with SGX disabled. 

    \begin{table}[h]
	\centering
	\footnotesize
	\caption{Experimental hardware setup}
	\begin{tabular}{| c | c | c | c |} \hline
		Scenarios & \multicolumn{2}{|c|}{Heterogeneous offloading} & Homogeneous offloading \\ \hline \hline
		CPU & \makecell{Neoverse-N1 \\(ARMv7)} & \makecell{Intel Xeon \\P-8370C} & \makecell{Intel Xeon \\CPU P-8370C} \\ \hline
		Cores & 2 & 2 & 2 \\ \hline
		Clock (GHz) & 2.3 GHz & 2.8 GHz & 2.8 GHz \\ \hline
		RAM & 8 GB & 16 GB & 16 GB \\ \hline
		Intel SGX & No & Yes & No \\ \hline
       Interconnect & \multicolumn{2}{|c|}{1 Gbps Ethernet} & 10 Gbps Infiniband \\ \hline
	\end{tabular}
	\label{t:setup}
    \end{table}
    
    \section{Performance breakdown of remote enclave calls for \monitor Monitor mode} \label{ase:Performance breakdown of remote enclave calls}
    The \monitor monitor mode uses several key mechanisms to transparently offload enclaves to an SGX-enabled machine, such as the ptrace APIs, the userspace page fault handling, and the CRIU's compel framework for extending virtual memory ranges, to name a few. Understanding these components interplay is essential to identify potential bottlenecks and better optimize \monitor. We inserted timestamps in the \monitor debug mode and printed the time consumed by each \monitor component into a log file. As reported in Table \ref{t:TimeConsumingCalls}, most \monitor internal API calls consume less than 50 us. There are two outliers: the compel API for injecting parasite code into the running program and the ptrace interface for page read/write operations, consuming 0.5 ms and 0.75 ms, respectively.

    The compel APIs (whether to steal the userfaultfd descriptor from the target or to extend the target’s virtual memory) take around 0.5 ms to complete. This latency can be attributed to the number of tasks needed to infect the victim process with the parasitic code, such as stopping the victim process, preparing the infection handler, executing the remote code, curing the victim, and finally, resuming the victim process. Fortunately, we don’t need to frequently invoke compel once the target process is launched. The ptrace system call, especially for memory page read/write, also has a high overall latency of approximately 0.75 ms per page (where each page is 4096 bytes). This increased latency can be attributed to various factors, including system call overhead and data movement. Interestingly, transferring pages across nodes took less time than PTRACE\_PEEKDATA/PTRACE\_POKEDATA.

    \begin{table*}[t]
    \centering
    \footnotesize
    \caption{Execution time breakdown of \monitor APIs in the cloud setting.}
    \begin{tabular}{| c | c |} \hline
          API & Average time taken \\ \hline \hline
        \textbf{Compel APIs}  &  \textbf{0.5 ms}\\ \hline
        UFFD APIs &  12 us \\ \hline
        PTRACE\_GETREGS | PTRACE\_SETREGS API  &  10 us\\  \hline
        PTRACE\_POKETEXT | PTRACE\_PEEKTEXT API & 6 us\\ \hline
        \textbf{PTRACE\_PEEKDATA | PTRACE\_POKEDATA API (per page)} & \textbf{0.75 ms} \\ \hline 
        REMOTE\_EXECUTE & 2 us \\ \hline
        VMA\_FROM\_REMOTE & 4 us \\ \hline
        VMA\_BUFFER\_HEADER & 20 us \\ \hline
        VMA\_BUFFER & 50 us \\ \hline
        VMA\_TRANS\_ACK & 10 us \\ \hline
        REMOTE\_REGS & 20 us \\ \hline
    \end{tabular}
    \label{t:TimeConsumingCalls}
    \end{table*}
    
    \section{Performance evaluation of Monitor mode} \label{ase:performance evaluation of monitor mode}
    We conducted comprehensive tests to evaluate the performance overhead of monitor mode on various applications. Our tests included SGX-DNet (an Intel SGX version of the Darknet machine learning library), Plinius (a secure machine learning framework that uses Intel SGX for secure training of neural network models and persistent memory for fault tolerance), two memory intensive benchmarks – bw\_mem and lat\_read (from an SGX ported lmbench) and a file encryptor, a data sealing programs from the Open Enclave SDK. We present the normalized results of offloading the enclave from a docker instance running on a non-SGX machine to an SGX-enabled machine node, with a baseline of running the vanilla enclave on an SGX-enabled docker instance alone. Figure.\ref{fig:normalized_perf}  shows the normalized result. We also reported the total execution time in Table.\ref{tab:exe_time}.

    \begin{table*}[t]
    \caption{Application Execution Time with \monitor Monitor mode.}
    \footnotesize
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        Application mode & SGX\_DNet & Plinius & bw-mem & lat-read & File\_Encryptor & Data\_Sealing \\ \hline
        Native Execution   & 116 s & 166 s   & 40153 ms & 46034 ms & 431 ms  & 450 ms      \\ \hline
        Enclave Offloading & 122 s & 168 s   & 40692 ms & 46597 ms & 1686 ms & 4986 ms     \\ \hline
    \end{tabular}
    \label{tab:exe_time}
    \end{table*}

    For applications with longer execution time (e.g., SGX-DNet and Plinius), \monitor does not bring significant performance overhead. For small applications with shorter execution time, the cross-node enclave offloading and control flow transformation cost dominate
    the overall execution time. Thus, applications such as file encryptor and data sealing have a larger normalized performance overhead. 

        \begin{figure}[htb]
            \centering
            \includegraphics[scale=2.5]{figures/Normalized_performance.png}
            \caption{ Normalized performance overhead of running offloaded enclave applications using \monitor monitor mode against local execution in the cloud setting.} 
            \label{fig:normalized_perf}
        \end{figure}

    \section{Profiling of number of system events for Monitor mode} \label{sse:system_call_events_monitor_mode}
    To better explain the performance overhead that varies among different benchmarks, we further measured several other metrics, such as the number of enclave calls executed, synchronization messages used, pages transferred, injected parasite code calls, and the number of ptrace calls during the evaluation process. Figure \ref{fig:client_instructions} illustrates the results.

    \begin{figure}[H]
    \centering
    \includegraphics[scale=1.1]{figures/perf_char_monitor.png}
    \caption{Profile of a number of system events running applications benchmarks.} 
    \label{fig:client_instructions}
    \end{figure}

    As one can see from Figure (a) of \ref{fig:client_instructions}, the number of migrations is higher for applications such as File-encryptor and Data-Sealing. Additionally, along with the migrations, the number of pages that often change is also higher with File-encryptor and Data-sealing applications. This leads to an increase in the number of compel parasitic injections and a higher number of ptrace calls, resulting in larger overhead for those applications.
    
    \section{Performance evaluation of RPC mode} \label{sse:Performance evaluation of RPC mode}
        \begin{figure}[htb]
            \centering
            \includegraphics[scale=0.7]{figures/Normalized_performance_rpc.png}
            \caption{Normalized performance overhead of running heterogeneous offloaded enclave applications using HARES RPC mode against local execution in the cloud setting.} 
            \label{fig:rpc_enclave_offloading}
        \end{figure}

    For evaluating the \monitor RPC mode, we used some of the applications that come along with the Open Enclave SDK and the results are shown in Figure. \ref{fig:rpc_enclave_offloading}. Although comparing the performance of heterogeneous offloading to homogeneous offloading is an apples-to-oranges comparison, as we have a weaker non-SGX machine for heterogeneous offloading (Neoverse-N1) and the network bandwidth is also slower in heterogeneous offloading with just a 1-Gigabyte bandwidth. \monitor's RPC based heterogeneous offloading despite being less powerful and slower networking speed, the performance overhead is relatively lower compared to the overhead of the \monitor Monitor mode. As one can see with File-encryptor whose normalized performance was comparable with homogeneous offloading and at times better with some applications like Data-sealing. This is mainly because the RPC mode of \monitor avoids time-taking APIs such as compel's parasitic injection, PTRACE, and mainly the page synchronization.
    
    
    \section{Profiling of serialization and deserialization cost for RPC mode} \label{sse:Profiling of serialization and deserialization cost for RPC mode}
    Similar to the profiling of the APIs for the \monitor Monitor mode in Section \ref{sse:system_call_events_monitor_mode}, this section breaks down the performance results for Figure \ref{fig:rpc_enclave_offloading}. The results are shown in Figure \ref{fig:perf_breakdown_rpc}. Networking performance significantly impacts the overall performance of the application, with a considerable amount of time being spent on transferring bytes back and forth. Another interesting observation is that serialization or deserialization takes very minimal time, in fact, just a few milliseconds.

    \begin{figure}[htb]
        \centering
        \includegraphics[scale=0.6]{figures/perf_breakdown_rpc.png}
        \caption{Performance breakdown of Applications with \monitor RPC mode} 
        \label{fig:perf_breakdown_rpc}
    \end{figure}
    
    \section{Security evaluation} \label{sse:Security evaluation}
    The main goal of Hares is to bridge the security hardware availability gap in a hybrid-CPU environment. Hares provides a distributed shared memory to transparently hide the memory inconsistency of different machine nodes while still being able to expose the confidential computation capability across the machine boundary. With \monitor, applications that run on a low-end non-SGX device can still utilize hardware extensions available on a remote node.
    When there is a need for confidential computation, Hares intercepts the ecall replaced by the instrumented debug instruction and then redirects the control flow to the remote enclave code. The target application will not be aware of anything different unless the remote enclave code tries to access data from the target application. Once there is a data access, the Hares monitor receives a
    page fault and retrieves the missing memory page from the original node. Note that a malicious program cannot prevent or intercept the page fault handling as the page fault handler runs within the \monitor code monitor’s address space. Whenever the confidential computation is finished, the control flow transfers back to the original node, and the target application can only see the encrypted
    memory pages. We have to note that a malicious operating system may try to compromise the Hares code monitor, as it runs entirely in the userspace. A possible solution to prevent this attack is to issue a remote attestation and check the integrity of the \monitor monitor before the remote ecall. We leave the implementation as future work.
    
    \subsection{Threat model}
    We assume a threat model that falls in line with the threat model of SGX. The adversary is believed to be in possession of super root privileges who has access to all the software and physical hardware. Hence, all the software including the instances of \monitor, Untrusted part of \monitor client, Operating System along with the hardware such as DRAM, other peripherals across two nodes are not trusted. We solely rely upon the SGX based CPU along with the enclave and trust the primitives of Intel SGX mechanism.

    Furthermore, we assume that the code running inside enclave is benign and does not voluntarily relinquish any secret keys or critical information. Enclave developers are expected to implement security techniques, such as the one discussed in \cite{Protecting, SGX-Defense}, to safeguard against potential vulnerabilities in the enclave binary interface. Additionally, we take no responsibility in addressing the side channel attacks upon SGX enclaves. With our system, known as Hares, when an enclave is offloaded onto a remote node, it inherits the standard security benefits provided by the standard SGX protection mechanisms. This ensures that sensitive operations within the enclave remain secure and isolated from potential threats.

    \subsection{CVE analysis}
    In this section, we will enumerate and comprehensively discuss the types of CVEs that can be protected by leveraging the SGX as offered by \monitor. The \monitor plays a pivotal role in mitigating prevalent weaknesses and vulnerabilities, such as CWE-798 (Use of Hard-coded Credentials), CWE-200 (Information Exposure), CWE-321 (Hardcoded Cryptographic Key), CWE-326 (Inadequate Encryption Strength), and numerous others. Sadly, these kinds of vulnerabilities still persist within the realm of IoT, yet they can be readily averted through the utilization of SGX, as offered by \monitor. The Table \ref{t:CVEs} lists some of the CVEs that could be mitigated by deploying \monitor.

    For example, CVE-2019-13604 represents a vulnerability arising from a short key weakness, enabling potential attackers to employ brute force techniques to extract the key used for image obfuscation, subsequently granting them access to decrypt the protected image. With \monitor, the encryption key can be initially generated within the enclave and securely sealed in protected storage for future encryption \& decryption of images. In this case, even if an attacker manages to compromise the host system, they will be unable to access the enclave or extract the encryption key, rendering the brute force attack ineffective. Similarly, consider CVE-2023-37468, where passwords are stored in clear text within the database, devoid of encryption. This vulnerability can also be averted by utilizing \monitor SGX to encrypt the passwords along with the database using the SGX based SQLite alike databases that \monitor support, thereby enhancing their security.

    Another commonly encountered attack is the Return-Oriented Programming (ROP) attack \cite{Remote-procedure-call}. In this type of attack, the attacker adeptly manipulates the stack, exploiting buffer overflow vulnerabilities in the program to execute arbitrary code. Notably, successful ROP attacks often require knowledge of the state of registers and memory addresses. However, SGX offers protection by encrypting the entire memory contents of the enclave program along with an additional feature of providing encrypted enclave code. This will thwart the buffer overflow vulnerabilities from exploiting the arbitrary code execution on enclave code unless specified through the allowed Ecall entries. Furthermore, It's important to note that while SGX itself can be vulnerable to ROP-based attacks stemming from memory corruption vulnerabilities within the code residing in an enclave as illustrated by \cite{ROP-paper}, our focus here is specifically on addressing the protection mechanisms of the \monitor enclave against memory vulnerabilities originating from untrusted code. This approach enables the prevention of CVEs such as CVE-2017-10720 and CVE-2023-25668.

    \begin{table*}[t]
    \centering
    \footnotesize
    \caption{CVEs that could be mitigated with \monitor}
    \begin{tabular}{| c | c |} \hline
          CWE & CVE \\ \hline \hline
        Buffer overflows & CVE-2017-10720, CVE-2017-10722, CVE-2023-25668\\ \hline  
        Hardcoded Credentials & CVE-2021-33220, CVE-2023-37468, CVE-2021-33219\\ \hline
        Information exposure & CVE-2019-0741 \\ \hline
        Hardcoded crytographic keys & CVE-2015-4080 \\ \hline
        Inadequate encryption strength & CVE-2019-13604, CVE-2019-13603 \\ \hline
    \end{tabular}
    \label{t:CVEs}
    \end{table*}

    \subsection{Remote Attestation}
    With the utilization of \monitor, remote attestation functions in a manner similar to other use cases. Initially, the host process, residing in the non-SGX node, receives the challenge from the remote attestation service. This challenge is subsequently decrypted and signed by the enclave in the SGX node, facilitated by the \monitor. Following the completion of enclave’s operation, the enclave report, along with the signature from the quoting enclave will be sent to the remote attestation service from the host process in non-SGX node. Finally, the host process in the non-SGX receives the verification report from the remote attestation service.

    
	% This is the standard bibtex file. Do not include the .bib extension in <bib_file_name>.
	% Uncomment the following lines to include your bibliography: 
	%\bibliography{<bib_file_name>}
	%\bibliographystyle{plainnat}   

	% This formats the chapter name to appendix to properly define the headers:
	\appendix

	% Add your appendices here. You must leave the appendices enclosed in the appendices environment in order for the table of contents to be correct.
	\begin{appendices}
		\chapter{First Appendix} \label{app:appendix_one}
			\section{Section one} \label{ase:app_one_sect_1}
			\section{Section two} \label{ase:app_one_sect_2}
		\chapter{Second Appendix} \label{app:appendix_two}
	\end{appendices}
 
    \bibliographystyle{plainurl}
    \bibliography{references}

\end{document}

%****************************************************************************
% Below are some general suggestions for writing your dissertation:
%
% 1. Label everything with a meaningful prefix so that you
%    can refer back to sections, tables, figures, equations, etc.
%    Usage \label{<prefix>:<label_name>} where some suggested
%    prefixes are:
%			ch: Chapter
%     		se: Section
%     		ss: Subsection
%     		sss: Sub-subsection
%			app: Appendix
%     		ase: Appendix section
%     		tab: Tables
%     		fig: Figures
%     		sfig: Sub-figures
%     		eq: Equations
%
% 2. The VTthesis class provides for natbib citations. You should upload
%	 one or more *.bib bibtex files. Suppose you have two bib files: some_refs.bib and 
%    other_refs.bib.  Then your bibliography line to include them
%    will be:
%      \bibliography{some_refs, other_refs}
%    where multiple files are separated by commas. In the body of 
%    your work, you can cite your references using natbib citations.
%    Examples:
%      Citation                     Output
%      -------------------------------------------------------
%      \cite{doe_title_2016}        [18]
%      \citet{doe_title_2016}       Doe et al. [18]
%      \citet*{doe_title_2016}      Doe, Jones, and Smith [18]
%
%    For a complete list of options, see
%      https://www.ctan.org/pkg/natbib?lang=en
%
% 3. Here is a sample table. Notice that the caption is centered at the top. Also
%    notice that we use booktabs formatting. You should not use vertical lines
%    in your tables.
% 
%				\begin{table}[htb]
%					\centering
%					\caption{Approximate computation times in hh:mm:ss for full order 						versus reduced order models.}
%					\begin{tabular}{ccc}
%						\toprule
%						& \multicolumn{2}{c}{Computation Time}\\
%						\cmidrule(r){2-3}
%						$\overline{U}_{in}$ m/s & Full Model & ROM \\
%						\midrule
%						0.90 & 2:00:00 & 2:08:00\\
%						0.88 & 2:00:00 & 0:00:03\\
%						0.92 & 2:00:00 & 0:00:03\\
%						\midrule
%						Total & 6:00:00 & 2:08:06\\
%						\bottomrule
%					\end{tabular}
%					\label{tab:time_rom}
%				\end{table}
% 
% 4. Below are some sample figures. Notice the caption is centered below the
%    figure.
%    a. Single centered figure:
%					\begin{figure}[htb]
%						\centering
%						\includegraphics[scale=0.5]{my_figure.eps}
%						\caption{Average outlet velocity magnitude given an average  
%				        input velocity magnitude of 0.88 m/s.} 
%						\label{fig:output_rom}
%					\end{figure}
%    b. Two by two grid of figures with subcaptions
%					\begin{figure}[htb]
%						\centering
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_1_1.eps}
%							\caption{Subcaption number one}
%							\label{sfig:first_subfig}
%						\end{subfigure}
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_1_2.png}
%							\caption{Subcaption number two}
%							\label{sfig:second_subfig}
%						\end{subfigure}
%
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_2_1.pdf}
%							\caption{Subcaption number three}
%							\label{sfig:third_subfig}
%						\end{subfigure}
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_2_2.eps}
%							\caption{Subcaption number four}
%							\label{sfig:fourth_subfig}
%						\end{subfigure}
%						\caption{Here is my main caption describing the relationship between the 4 subimages}
%						\label{fig:main_figure}
%					\end{figure}
%
%----------------------------------------------------------------------------
%
% The following is a list of definitions and packages provided by VTthesis:
%
% A. The following packages are provided by the VTthesis class:
%      amsmath, amsthm, amssymb, enumerate, natbib, hyperref, graphicx, 
%      tikz (with shapes and arrows libraries), caption, subcaption,
%      listings, verbatim
%
% B. The following theorem environments are defined by VTthesis:
%      theorem, proposition, lemma, corollary, conjecture
% 
% C. The following definition environments are defined by VTthesis:
%      definition, example, remark, algorithm
%
%----------------------------------------------------------------------------
%
%  I hope this template file and the VTthesis class will keep you from having 
%  to worry about the formatting and allow you to focus on the actual writing.
%  Good luck, and happy writing.
%    Alan Lattimer, VT, 2016
%
%****************************************************************************





